**寻找成熟模型：**

step1 ：被选中的IV初始化其任务，将初始化的Actor网络参数记作$\theta^{0}$.

step2:    IV收集当前时刻的环境信息$s_{t}$,并根据Actor来执行当前的动作$a_{t}$，并记录执行后的环境信息$s_{t+1}$以及对应的奖惩值（reward）$r_{t+1}$。将这些信息按照$<s_{t},a_{t},s_{t+1},r_{t+1}>$的四元组形式进行记录。

step3：A-C网络会以mini-batch的形式采样记录并送入网络中进行处理，得到一个用来更新Actor网络的梯度$g_{i}^{t}$,IV还会计算当前网络的表现水平（Performance Index），记作$PI_{i}^{t}$，随后将$g_{i}^{t}$和$PI_{i}^{t}$上传给RSU。

step4：RSU会等待IV上传对应的梯度更新和PI值，直到达到一次训练的时间阈值$T^{Update}$或是确认收到了来自所有被选中IV的梯度和PI值。此时RSU会根据表现值计算各个IV梯度的权值并按照加权算术平均求出此轮更新的平均梯度，随后将这些梯度信息交付给所有的IV（包括未选中的）。

step5：IV通过得到的平均梯度信息更新自己的Actor网络。

step6：RSU在交付平均梯度后，需要开始下一轮的节点选择过程。重复step1-step6直到某个IV的模型达到收敛（计算出PI超过某个阈值$TC(Terminate \space Conditon)$）。

**成熟模型分发与更新：**

step7：一旦某个IV计算得出自己的PI超出TC，即得到一个成熟模型，该模型就可以被用作当前辖区的暂定最优模型（TBM，Temporary Best Model）。此时IV不再上传最后一次的梯度更新，而是直接上传整个Actor网络的参数$\theta^{TBM}$给RSU。

step8：当RSU收到$\theta^{TBM}$后，会将这个模型以及其PI值暂时保存，接着会向仍未收敛的IV发送$\theta^{TBM}$。

step9：其余的IV收到$\theta^{TBM}$后，首先计算当前训练的表现与TC的差距，得到聚合权值，接着按照权值将$\theta^{TBM}$聚合到自己的Actor参数中。

step10：一旦有模型收敛，首先计算模型的PI并和当前得到的成熟模型的PI做比较，如果自身的模型PI较大，则上传该模型给RSU。

step11：RSU在这一阶段收到来自IV的计算模型后，会再次比较PI，较大的模型成为最新的全局模型。



**两个关键点**

如何计算模型的PI？

在强化学习中，由于其训练的目标在于计算
$$
max \space \sum^{S}_{i=0} r_{t-i}
$$
即过去的多个轮次中的奖励函数和值的最大值，所以模型的表现和这一和值相关，我们将这一和值称为回报函数，记作$Re(t) = \sum^{S}_{i=0} r_{t-i} $。很明显，一个被训练了的模型的PI可以由此时的$Re(t)$决定，即$PI^{t}_{a} = sig(Re_{a}(t))$,其中$sig$是归一化函数。
